<!doctype html>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>PM - Maryam Saberi</title>
	<link rel="stylesheet" type="text/css" href="css/style.css">
	<link rel="stylesheet" type="text/css" href="css/mobile.css" media="screen and (max-width : 568px)">
	<script type="text/javascript" src="js/mobile.js"></script>
</head>
<body>
	<div id="header">
			<ul id="navigation">
			<li class="selected"><a href="index.html">home</a></li>
			<li><a href="projects.html">projects</a></li>
			<li><a href="industry.html">UXR</a></li>
			<li><a href="teaching.html">teaching</a></li>
			<li><a href="publication.html">publications</a></li>
			</ul>
	</div>
	<div id="body">
		<div>
			<img src="images/RPS-users.jpg" alt="">
			<div class="article">
				<h2>A Scenario for Nonverbal Interaction</h2>
				<img src="images/RPS-1.jpg" alt="">
				<p class="just">
					My designed system focuses on dynamic interaction with users because I believe an interactive environment is a necessity in creating a stronger perception of personality traits. The test case scenarios were designed to reduce the problem set while developing and testing the system. The goal was to create an easy-to-learn and engaging scenario that provides an interactive environment with a minimum dialog. 
				</p>
		</div>
		<div>
			<ul>
				<li class="solidBorder" >
					<a href="Sensor.html" class="figure"><img src="images/RPS-rps.jpg" alt=""></a>
					<div class="divLowPadding just">				
						<p class="pLowLineheight just">	
							<strong style="font-size:17px"> Rock-Paper-Scissors Game</strong>
							</br></br>
							During the game, the virtual character plays multiple rounds of the rock-paper-scissors game with the user. A GUI (graphical user interface) is used for synchronizing the RPS game. 
							</br></br>
							Gestures and poses used in Rock Paper Scissor Scenario are mainly pre-defined animations that can be grouped, blended and partially dynamically controlled. Some of the poses and gestures used are provided by Smartbody animation toolkit and some are generated by iVizlab’s animation team. 
							</br></br>
							Some of the parameters of the animations such as speed, starting and ending time of the movements can be controlled dynamically in the simulation. Personality type, which is an input parameter to this module, affects the speed of the gestures. For example, a highly extravert character shows faster rock, paper, and scissors hand gestures.
						</p>
					</div>
				</li>
			</ul>
		</div>
		<div>
			<ul>
				<li class="solidBorder" >
					<a href="Sensor.html" class="figure"><img src="images/RPS-ms.png" alt=""></a>
					<div class="divLowPadding just">				
						<p class="pLowLineheight just">	
							<strong style="font-size:17px"> Mine-Sweeper Game</strong>
							</br></br>
							The user’s objective is to reach the target while avoiding the mines. The character tries to guide the user in the field using gestures such as pointing. All the possible destinations for the user are marked.
							</br></br>
							Three types of minesweeper like scenarios were designed to investigate the best way to provide an infrastructure for revealing personality through nonverbal behaviour. The first version involved having the character as a passive observer of users’ movements in a field while showing emotional reactions to what the user does. In the passive version, no gestures were involved. 
							</br></br>
							The second version of the scenario involved interactive, cooperative 3D virtual characters that used pointing and gestures to help the users meet the goal of the interaction. In the third version, the character was randomly selected to be not cooperative. 

						</p>
					</div>
				</li>
			</ul>
		</div>
		<div>
			<ul>
				<li class="solidBorder" >
					<a href="Sensor.html" class="figure"><img src="images/RPS-ms3.jpg" alt=""></a>
					<div class="divLowPadding just">				
						<p class="pLowLineheight just">				
							In the passive scenario, the character is a passive observer of users’ actions while showing emotional reactions to what the user does. In an interactive scenario, the character uses pointing and other gestures to help the users meet the goal of the interaction.
							</br></br>
							A prototype using the scenario with human-human interaction is performed first, to evaluate the scenario and extract the range needed for motion capture and 3D animation. I designed a prototype in which I recorded the interaction of two humans instead of a 3D virtual character and a human. 	
							</br></br>
							The recorded human behavioural acts were analyzed and used as a reference for generating the 3D virtual characters’ nonverbal behaviour. Different scenarios are prototyped with humans and compared to investigate which one is the best candidate for the actual experiment.
						</p>
					</div>
				</li>
			</u>
		</div>
		<div id="footer">
		<div>
			<p>&copy; Maryam Saberi</p>
			<ul>
				<li><a href="https://twitter.com/maryamsaberi" id="twitter">twitter</a></li>
				<li><a href="https://www.facebook.com/maryam.saberi.184" id="facebook">facebook</a></li>
				<li><a href="https://www.linkedin.com/in/maryam-saberi-ph-d-b1652224/" id="linkedin">linkedin</a></li>
			</ul>
		</div>
	</div>
</body>
</html>

