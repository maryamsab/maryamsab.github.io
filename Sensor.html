<!doctype html>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>IVH - Maryam Saberi</title>
	<link rel="stylesheet" type="text/css" href="css/style.css">
	<link rel="stylesheet" type="text/css" href="css/mobile.css" media="screen and (max-width : 568px)">
	<script type="text/javascript" src="js/mobile.js"></script>
</head>
<body>
<div id="header">
		<ul id="navigation">
			<li ><a href="index.html">home</a></li>
			<li class="selected"><a href="projects.html">projects</a></li>
			<li><a href="teaching.html">teaching</a></li>
			<li><a href="publication.html">publications</a></li>
		</ul>
	</div>
	<div id="body">
		<div>
			<div class="article">
				<h2>Interactive Virtual Human</h2>
					
					<img src="images/setup.jpg" alt="">
					<p class="just">
						For a virtual character to come across as responsive and emotive in real-time, it must appear to first sense the user’s movements, facial expression, hand gestures, and even intent (via say in a rock-paper-scissors game), and then process those sensory signals. To do this I designed, implemented, and tested several sophisticated real-time bio, gesture, and movement sensors in the system. A data glove sensor is used to capture the user’s hand gestures. 
					</p>
					
					<img src="images/design1.jpg" alt="">
					<p class="just">
						The system supports the capture, recognition, control of movement data. Thus, through sessions of expert interview and focus groups, we explored how to design an infrastructure for all the components of the system to works to gether.
					</p>			
					
					<img src="images/design2.jpg" alt="">
					<p class="just">
						The final design included a functional and modular structure which connects many applications such as animation system, and sensors. The moduler structure let other designers easily add new sesnors and animation frameworks.
					</p>
					
					<iframe width="375" height="293" src="https://www.youtube.com/embed/LFDWFitrrW0" frameborder="0" allowfullscreen=""></iframe>				
					<p class="just">
						Next, is the video of functional setup with all the sensors such as Kinect and webcam working in real-time and in parallel.
					</p>
					
					<iframe width="375" height="293" src="https://www.youtube.com/embed/O2SUUo55lXM" frameborder="0" allowfullscreen=""></iframe>				
					<p class="just">
						A Microsoft Kinect 3D camera was used to locate the user’s head to guide the virtual character’s gazes. In addition, the character adjusts his personal space with the user as she gets too close. 
					</p>
					
					<iframe width="375" height="293" src="https://www.youtube.com/embed/I-sZEyvtsXk" frameborder="0" allowfullscreen=""></iframe>
					<p class="just">
						Various sensors, such as a Kinect 3D camera and overhead cameras were used to send streams of input such as users’ coordination in the space, users’ heights or environmental information such as noise and light. The character reacts dynamically and in real-time to these inputs. The SHORE application developed by Fraunhofer research center, was used to receive the input stream from a webcam and forward information such as emotional expression of the user’s facial expressions (and age, sex) to RealAct. The electromyography (EMG) sensor was used to measure activity of the facial muscles to detect the user’s smile in real-time. 
					</p>
					
				
			</div>
		</div>
	</div>
	<div id="footer">
		<div>
			<p>&copy; Maryam Saberi</p>
			<ul>
				<li><a href="https://twitter.com/maryamsaberi" id="twitter">twitter</a></li>
				<li><a href="https://www.facebook.com/maryam.saberi.184" id="facebook">facebook</a></li>
				<li><a href="https://www.linkedin.com/in/maryam-saberi-ph-d-b1652224/" id="linkedin">linkedin</a></li>
			</ul>
		</div>
	</div>
</body>
</html>
